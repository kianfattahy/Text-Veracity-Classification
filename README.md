# Text Veracity Classification

## Overview
This project applies Natural Language Processing (NLP) methods to classify textual information about aquatic animals into true or false statements. It examines the effectiveness of various preprocessing techniques and linear classifiers in improving the prediction accuracy of the classification models.

## Problem Setup
An experiment was conducted to evaluate how different preprocessing methods and linear classifiers affect the performance of text classification models on a dataset of true and false statements about aquatic animals. The dataset was split into 75% training and 25% testing sets, and a linear classifier was used to predict the truthfulness of each statement.

## Experimental Procedure
The dataset comprised true and false facts generated by GPT 3.5. Three preprocessing techniques were applied: stemming, lemmatization, and removal of stop words, along with the use of POS tags for lemmatization. The performance was measured using the F1 score.

### Preprocessing Techniques
- **Stemming**: Reduced words to their root forms to create a consistent vocabulary.
- **Lemmatization**: Retained more of the original word forms and utilized POS tags for more accurate results.
- **Stop words removal**: Eliminated common words that add no significant meaning to the text.

### Classification Methods
- **Support Vector Machine (SVM)**: Explored the effectiveness of the linear model in high-dimensional spaces.
- **Naive Bayes**: Tested for its ability to handle nuanced data.
- **Logistic Regression**: Analyzed the relationship between features and the output using a logistic curve.

Hyperparameters were fine-tuned using GridSearchCV, optimizing for the best model configurations.

## Results and Conclusions
The results demonstrated varied performance across different preprocessing techniques and linear classifiers. The Support Vector Machine (SVM) with lemmatization achieved the highest F1 score on the test set, indicating its effectiveness in handling the classification task. Below is a detailed table of the preprocessing approaches, selected models, optimal parameters, best cross-validation (CV) F1 scores, and testing F1 scores:

| Preprocessing Approach | Selected Model     | Optimal Parameters                                  | Best CV F1 Score | Testing F1 Score |
|------------------------|--------------------|------------------------------------------------------|------------------|------------------|
| baseline               | SVM                | `modelClassifier_alpha: 0.001, modelClassifier_penalty: l2`          | 0.9586           | 0.9189           |
| baseline               | Naive Bayes        | `modelClassifier_alpha: 0.1`                         | 0.9433           | 0.9449           |
| baseline               | Logistic Regression| `modelClassifier_alpha: 0.1`                         | 0.9283           | 0.9414           |
| stem                   | SVM                | `modelClassifier_alpha: 0.001, modelClassifier_penalty: l2`          | 0.9504           | 0.9315           |
| stem                   | Naive Bayes        | `modelClassifier_alpha: 0.1`                         | 0.9425           | 0.9577           |
| stem                   | Logistic Regression| `modelClassifier_alpha: 0.1, modelClassifier_penalty: l2` | 0.9577           | 0.9419           |
| lemmatize              | SVM                | `modelClassifier_alpha: 0.001, modelClassifier_penalty: l2`          | 0.9492           | 0.9492           |
| lemmatize              | Naive Bayes        | `modelClassifier_alpha: 0.1`                         | 0.9281           | 0.9275           |
| lemmatize              | Logistic Regression| `modelClassifier_alpha: 0.1`                         | 0.9470           | 0.9446           |
| bigrams                | SVM                | `modelClassifier_alpha: 0.001, modelClassifier_penalty: l2`          | 0.9437           | 0.9296           |
| bigrams                | Naive Bayes        | `modelClassifier_alpha: 0.1`                         | 0.9408           | 0.9315           |
| bigrams                | Logistic Regression| `modelClassifier_alpha: 0.1`                         | 0.9148           | 0.9315           |

Stemming notably improved the F1 score of the Naive Bayes model, while lemmatization was most beneficial for the SVM, enhancing its test F1 score to 0.9492. The use of bigrams increased the model's ability to handle more nuanced and detailed data, which was particularly evident in the SVM's performance.

However, the application of bigrams did not result in significant improvements for the Naive Bayes model. Logistic Regression showed the least variance in F1 scores across different preprocessing methods but performed optimally with the baseline data.

The study's limitations include the AI-generated nature of the dataset, the exclusive use of linear classifiers, and the limited range of hyperparameters tested, which may have affected the generalizability of the models to real-world data.

## Limitations of the Study
- The dataset was generated by GPT 3.5, which may introduce biases unique to AI-generated text.
- Only linear classifiers were tested, potentially missing non-linear patterns in the data.
- A limited number of hyperparameters were explored, possibly overlooking more optimal configurations.
- The models may be too tailored to the training data, risking overfitting and reduced accuracy on unseen data.

## Future Work
Future studies should explore deep learning techniques to discover non-linear patterns, test a wider range of hyperparameters, and evaluate the models on real-world data to ensure the results are not limited to AI-generated text.

## How to Use This Project
1. Clone the repository.
2. Install the required libraries mentioned in `requirements.txt`.
3. Run the preprocessing scripts to prepare the dataset.
4. Execute the training scripts with different classifiers.
5. Use GridSearchCV for hyperparameter tuning.
6. Evaluate the models using the test dataset.
7. Review the results and adjust the preprocessing or models as needed.
